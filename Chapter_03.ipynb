{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf5b7ad",
   "metadata": {},
   "source": [
    "# Chapter 03 : 로지스틱 회귀 모델\n",
    "## 사전 지식\n",
    "### 최대하강법\n",
    "- 주어진 비용 함수의 지역 최솟값(local minimum)이나 전역 최솟값(global minimum)을 구하는 것은 수학적 최적화 과정 중 하나\n",
    "- 경사 하강법 : 수학적 최적화 과정을 수행하는 방법의 하나\n",
    "- 경사 or 그레이디언트의 역방향으로 입력값을 차례대로 이동하며 최소의 목푯값을 달성하는 모든 방법\n",
    "- 최대하강법(steepest descent), 뉴턴법(Newton's method), BFGS 등 다양한 알고리즘이 존재\n",
    "- 최대하강법 : 이터레이션마다 해당 점의 그레이디언트를 구하고 그 역방향으로 그레이디언트의 상수 배만큼 좌표를 이동하며 지역 최솟값을 찾는 최적화 알고리즘\n",
    "\n",
    "#### 최대하강법 절차\n",
    "1. 사전 정의된 스텝 사이즈(step size) $\\Delta$가 주어지고 초기 시작점 $x_0$에서 시작\n",
    "2. 최대 이터레이션에 도달하거나 허용 오차 내에서 $f(x_{t + 1}) \\approx x_t-\\Delta f'(x_t)$를 반복 계산하고 업데이트\n",
    "- $\\Delta$는 계산 결과를 반영하는 정도를 나타냄 $\\rightarrow$ 학습률(learning rate)\n",
    "- 최대(steepest)의미는 2차원 공간에 최대하강법을 적용하고 이를 그래프에서 확인할 경우 더욱 분명하게 확인할 수 있음\n",
    "\n",
    "### 최대하강법 구현하기\n",
    "- 함수의 형태가 매우 복잡하여 최솟값을 직접 유도할 수 없는 복잡한 경우에 해당 알고리즘을 적용함\n",
    "- ex) $f(x) = x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0adb0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028bb93a",
   "metadata": {},
   "source": [
    "- 초기점(initial point)인 x0, 스텝 사이즈 Delta, 허용 오차 Tolerance 값을 정의\n",
    "- 허용 오차 Tolerance : 정지 조건에 필요한 값, 이터레이션 중 파라미터 업데이트의 크기가 Tolerance보다 작다면 학습을 중지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cd99dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, Delta, Tolerance = 2, 0.4, 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecd159",
   "metadata": {},
   "source": [
    "- 그레이디언트를 계산하여 값을 반환하는 함수를 정의\n",
    "- 그레이디언트의 함수식 정의는 딥러닝과 같이 체인 룰(chain rule)이 수많은 횟수로 중첩되어 적용되었을 때 매우 유용\n",
    "- ex) $f(x) = x^2 \\rightarrow f'(x) = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f064aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da314a5a",
   "metadata": {},
   "source": [
    "- 이터레이션별 상황을 추적하고 이터레이션을 수행하는 코드를 구현\n",
    "- 학습 히스토리를 저장하고자 리스트 xs를 정의하고 이터레이션별 x값을 저장\n",
    "- ex) f'(x) = 2x $\\rightarrow$ $f(x_{t + 1}) \\approx x_t-\\Delta f'(x_t)$ $\\rightarrow$ $f(x_{t + 1}) = x_t-\\Delta 2x_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb1222ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이터레이션 횟수 :  14\n",
      "x_curr의 값 :  0.0015672832819200039\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "x_prev = x0\n",
    "\n",
    "for cnt in range(100):\n",
    "    cnt += 1\n",
    "    xs.append(x_prev)\n",
    "    x_curr = x_prev - 2 * Delta * grad(x_prev)\n",
    "    \n",
    "    diff = np.abs(x_curr - x_prev)\n",
    "    if (diff <= Tolerance):\n",
    "        break\n",
    "        \n",
    "    x_prev = x_curr\n",
    "    \n",
    "print(\"이터레이션 횟수 : \", cnt)\n",
    "print(\"x_curr의 값 : \", x_curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d678f",
   "metadata": {},
   "source": [
    "- 누적한 히스토리(history)를 확인\n",
    "- 결과 그래프 : 이터레이션 횟수에 따른 졈의 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb6d0d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a30a01e460>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(cnt)], [0] * cnt, 'k', label = 'Optimum Parameter')\n",
    "plt.plot([i for i in range(cnt)], xs, 'r', label = 'Steepest Decsent')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf00d5",
   "metadata": {},
   "source": [
    "- 추정한 값이 해 위아래로 변하며 0에 가까워지는 것을 확인할 수 있음\n",
    "- $\\Delta$를 작게 설정하면 단방향으로 0으로 수렴하는 결과를 얻을 수도 있음\n",
    "- $\\Delta$를 직접 설정해야 한다는 점에서 부담과 여러 번의 시행착오가 있을 수 있음\n",
    "- $\\Delta$ 선택은 매우 중요\n",
    "- $\\Delta$를 너무 작게 잡으면 이터레이션마다 변동폭이 너무 적어 수렴 속도가 느려질 수 있음\n",
    "- $\\Delta$를 너무 크게 설정하면 진동이 발생할 수 있고, 마찬가지로 수렴 속도가 느려질 수 있음\n",
    "\n",
    "뉴턴법 : 최대하강법에서의 $\\Delta$를 상수가 아니라 2계 도함수의 역수인 $\\frac{1}{f''(x)}$로 택하는 기법\\\n",
    "$\\rightarrow$ 좋은 $\\Delta$값을 이터레이션마다 사용자에게 지정해주기 위한 것으로 생각 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac1277",
   "metadata": {},
   "source": [
    "### 컨벡스 함수\n",
    "- 함수 $f$가 컨벡스 함수(convex function)라면 지역 최솟값(local minimum)이 전역 최솟값(global minimum)이 되는 매우 좋은 성질이 있음\n",
    "- 컨벡스 최적화(convex optimization) : 전역 최솟값을 찾는 과정 $\\rightarrow$ 유일한 해가 도출됨\n",
    "- 머신러닝의 경우 비용 함수를 최소화하는 과정을 거치게 되는데, 이때 비용 함수가 컨벡스 함수인지가 중요\n",
    "- 컨벡스 함수 : $f(\\lambda x + (1 - \\lambda)y)\\leq \\lambda f(x) + (1 - \\lambda)f(y)\\;$ ($f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$이 모든 $x, y \\in \\mathbb{R}^n$과 $\\lambda \\in [0, 1]$에 대하여 만족)\n",
    "- 임의의 두 점 (x, f(x)), (y, f(y))를 선택\n",
    "- 두 점을 선분으로 이을 경우 그 선분이 그 사이의 구간에서 항상 f(x)보다 크거나 같음\n",
    "- 미분 가능성의 가정이 필요하지 않음\n",
    "\n",
    "#### 컨벡스 함수의 성질\n",
    "1. $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$과 $g : \\mathbb{R}^n \\rightarrow \\mathbb{R}$이 모두 컨벡스 함수일 때 f + g도 컨벡스 함수\n",
    "2. 함수 $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$이 미분 가능한 컨벡스 함수라 가정했을 때, 임의의 $x, y \\in \\mathbb{R}^n$에 대하여 부등식 $f(y)\\geq f(x) + \\nabla f(x)^T(y - x)$이 성립하며 이의 역 또한 성립\n",
    "3. $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$가 컨벡스 함수이며 미분 가능하다고 가정할 때 $\\nabla f(x^*) = 0$이면 $f(x^*)$는 $f$의 전역 최솟값 $\\rightarrow f(y)\\geq f(x^*) + \\nabla f(x^*)^T(y - x^*) = f(x^*)$\n",
    "4. 두 번 이상 미분가능하며 두 번 미분 후에도 연속인 함수 $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$가 있고 헤시안 $\\nabla^2f(x)$가 모든 $x \\in \\mathbb{R}^n$에 대하여 양의 준정부호 행렬이면 $f$는 컨벡스 함수\n",
    "\n",
    "#### 컨벡스 함수와 컨케이브 함수\n",
    "- 함수 $g : \\mathbb{R}^n \\rightarrow \\mathbb{R}$이 모든 $x, y \\in \\mathbb{R}^n$과 $\\lambda \\in [0, 1]$에 대하여 $g(\\lambda x + (1 - \\lambda)y) \\geq \\lambda g(x) + (1 - \\lambda)g(y)$을 만족하면 $g$를 컨케이브 함수(concave function)라 함\n",
    "- $g$가 컨케이브 함수인 것과 $-g$가 컨벡스 함수인 것은 동치\n",
    "- 선형 함수는 컨벡스 함수인 동시에 컨케이브 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae76c4",
   "metadata": {},
   "source": [
    "## 1. 로지스틱 회귀 모델이란?\n",
    "- 로지스틱 회귀(logistic regression)는 분류 문제(classification task)를 해결하는 가장 기본적인 머신러닝 모델의 하나\n",
    "- 로지스틱 회귀라는 이름에도 불구하고 회귀가 아닌 분류에 사용\n",
    "- 로짓 회귀(logit regression), MaxEnt 분류(maximum-entropy classification), 로그-선형 분류기(log-linear classifier) 등으로 부르기도 함\n",
    "- 목표 클래스의 발생 확률을 로지스틱 함수(logistic function)로 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584914d",
   "metadata": {},
   "source": [
    "## 2. 로지스틱 회귀 모델 구현하기\n",
    "### 로지스틱 회귀 모델의 기본 이론\n",
    "- 로지스틱 회귀 모델 : 주어진 이진 목푯값 y의 클래스 레이블 값이 1이 나올 확률 p(y)와 피처 벡터 x 사이의 관계를 모델링하는 기법\n",
    "\n",
    "#### 로지스틱 회귀 모델 생성 과정\n",
    "1. 피처 벡터 $x_i$가 외부에서 결정(exogenously determined)됨\n",
    "2. $y_i$는 $x_i$와 $w$에 조건부로 정해지며 $y_i$가 1이 될 확률이 $p(y_i|x_i,\\,w)$로, 0이 될 확률이 $1 - p(y_i|x_i,\\,w)$로 주어지는 베르누이 분포(Bernoulli distribution)를 따름\n",
    "    - $w$는 추정하고자 하는 값이지만, 상수로 이미 주어져 있다고 가정\n",
    "3. $p(y_i|x_i,\\,w)$에 따라 $y_i$가 결정됨\n",
    "\n",
    "베르누이 분포\n",
    "- 가장 간단한 형태의 분포 중 하나\n",
    "- 특정한 사건의 발생 여부를 0과 1으로 모델링하는 것\n",
    "- 특정한 일이 발생한 경우 : 1\n",
    "- 특정한 일이 발생하지 않은 경우 : 0  \n",
    "\n",
    "로지스틱 회귀 모델 생성 과정 2에서 $x_i$와 $p(y_i|x_i,\\,w)$의 관계 모델링  \n",
    "- 절편(intercept)을 모델에 포함하고자 $x_i,\\,0 = 1$로 정의, 피처 행렬 X에 포함된다고 가정  \n",
    "- $log\\frac{p(y_i|x_i,\\,w)}{1 - p(y_i|x_i,\\,w)} = w_0 + w_1x_{i,1} + \\dots + w_px_{i,p} = \\displaystyle\\sum_{j = 0}^{p} w_jx_{i,j}$  \n",
    "- 우변의 선형 결합 $\\displaystyle\\sum_{j = 0}^{p} w_jx_{i,j}$이 커질수록 좌변에서의 $p(y_i|x_i,\\,w)$ 또한 커지나 $p(y_i|x_i,\\,w)$ 값이 0과 1 사이로 제한되므로 모델링하는 것  \n",
    "\n",
    "피처 벡터 $x_i$를 행벡터로, 추정 파라미터 벡터 w를 열벡터로 표현  \n",
    "- $log\\frac{p(y_i|x_i,\\,w)}{1 - p(y_i|x_i,\\,w)} = x_iw$\n",
    "\n",
    "위의 식을 $p(y_i|x_i,\\,w)$에 대해 풀음  \n",
    "- $p(y_i|x_i,\\,w) = \\frac{1}{1 + e^{-x_iw}} = \\frac{e^{x_iw}}{1 + e^{x_iw}}$  \n",
    "- $1 - p(y_i|x_i,\\,w) = \\frac{1}{1 + e^{x_iw}}$  \n",
    "\n",
    "$\\rightarrow p(y_i|x_i, \\, w) = \\sigma(x_iw)$  \n",
    "\n",
    "과정 3에서는 과정 2에서 정의한 확률에 따라 $y_i$를 결정  \n",
    "모델 학습 단계에서는 실제 데이터에서의 $y_i$값과 모델이 예측하는 $p(y_i|x_i,\\,w)$값을 비교하여 손실 함수(loss function)를 정의할 수 있음  \n",
    "- 손실 함수 : 샘플 하나에 대한 모델의 오차의 크기를 나타내는 함수\n",
    "- 손실 함수를 전체 샘플에 대해 누계할 때 비용 함수가 얻어지는 것으로 생각 가능\n",
    "  \n",
    "  \n",
    "$J(w) = -(1 - y_i)log(1 - p(y_i|x_i,\\,w))-y_ilogp(y_i|x_i,\\,w)$\n",
    "- $y_i$가 1일 때 $p(y_i|x_i,\\,w)$가 1에 가까울수록 작은 손실이 부과되고 0일 때 $p(y_i|x_i,\\,w)$가 0에 가까울수록 작은 손실이 부과되도록 표현  \n",
    "\n",
    "$J(w) = \\displaystyle\\sum_{i = 1}^{n} J_i(w) = \\displaystyle\\sum_{i = 1}^{n} (-(1 - y_i)log(1 - p(y_i|x_i,\\,w))-y_ilogp(y_i|x_i,\\,w)) = -\\displaystyle\\sum_{i = 1}^{n} \\Bigg( y_ilog\\frac{p(y_i|x_i,\\,w)}{1 - p(y_i|x_i,\\,w)} + log(1 - p(y_i|x_i,\\,w))\\Bigg )$  \n",
    "$J(w) = \\displaystyle\\sum_{i = 1}^{n} (log(1 + e^{x_iw}) - y_ix_iw)$  \n",
    "- 모든 i에 대하여 $log(1 + e^{x_iw})$와 $-y_ix_iw$ 모두가 컨벡스 함수  \n",
    "$\\rightarrow$  선형 결합인 비용 함수 $J(w)$ 또한 컨벡스 함수  \n",
    "$\\rightarrow$ 경사 하강법으로 찾은 해를 $J(w)$를 전역적으로 최소화함\n",
    "\n",
    "#### 단변수 로지스틱 회귀 모델 최적화하기\n",
    "- 피처 $x_i$가 앞과 다르게 벡터값이 아닌 스칼라값으로 주어졌다고 가정  \n",
    "  \n",
    "$log \\frac{p(y_i|x_i,\\,w)}{1 - p(y_i|x_i,\\,w)} = w_0 + w_1x_i$  \n",
    "비용 함수 $J(w) = \\displaystyle\\sum_{i = 1}^{n}\\Big (log(1 + e^{w_0 + w_1x_1}) - y_i(w_0 + w_1x_i) \\Big )$  \n",
    "$w_0$에 대해 비용 함수의 편미분 값 $\\frac{\\partial J(w)}{\\partial w_0} = \\displaystyle\\sum_{i = 1}^{n} \\bigg ( \\frac{e^{w_0 + w_1x_i}}{1 + e^{w_0 + w_1x_i}} - y_i \\bigg )$ $\\rightarrow$ $\\frac{\\partial J(w)}{\\partial w_0} = \\displaystyle\\sum_{i = 1}^{n} \\big ( p(y_i|x_i,\\,w) - y_i\\big )$  \n",
    "$w_1$에 대해 비용 함수의 편미분 값 $\\frac{\\partial J(w)}{\\partial w_1} = \\displaystyle\\sum_{i = 1}^{n} x_i \\bigg ( \\frac{e^{w_0 + w_1x_i}}{1 + e^{w_0 + w_1x_i}} - y_i \\bigg )$ $\\rightarrow$ $\\frac{\\partial J(w)}{\\partial w_1} = \\displaystyle\\sum_{i = 1}^{n} x_i \\big (p(y_i|x_i,\\,w) - y_i \\big )$  \n",
    "\n",
    "#### 뉴턴법을 이용하여 다중 로지스틱 회귀 모델 최적화하기\n",
    "행렬 형태의 수식을 전개\n",
    "피처 행렬 $X = \\begin{pmatrix} 1 & \\cdots & x_{1,p} \\\\\n",
    "                               \\vdots  & \\ddots & \\cdots \\\\\n",
    "                               1 & \\cdots & x_{n,p} \\end{pmatrix}$  \n",
    "뉴턴법의 이터레이션에서는 다음과 같이 2계 도함수인 헤시안을 학습률로 지정  \n",
    "$w^{(t + 1)} = w^{(t)} - \\Bigg [ \\bigg ( \\frac{\\partial^2J(w)}{\\partial w \\partial w^T} \\bigg ) \\frac{\\partial J(w)}{\\partial w} \\Bigg ]$  \n",
    "표기법의 편의상 벡터 $p를 p = [p_1, p_2, \\dots, p_n]$으로 정의하며 $p_i = p(y_i|x_i,\\,w)$라 함  \n",
    "비용 함수의 1계 도함수 부분을 행렬 형태로 표현  \n",
    "$\\frac{\\partial J(w)}{\\partial w} = \\displaystyle\\sum_{i = 1}^{n} x_i(p(y_i|x_i,\\,w) - y_i) = X^T(p - y)$  \n",
    "  \n",
    "2계 도함수 부분을 계산하고자 W를 대각 행렬로 정의  \n",
    "$W = diag[p_i(1 - p_i)]_{i = 1}^{n} = \\begin{pmatrix}\n",
    "                                      p_1(1 - p_1) & 0 & \\cdots & 0 \\\\\n",
    "                                      0 & p_2(1 - p_2) & \\cdots & 0 \\\\\n",
    "                                      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                      0 & 0 & \\cdots & p_n(1 - p_n)\n",
    "                                      \\end{pmatrix}$  \n",
    "  \n",
    "비용 함수의 헤시안 $H = \\frac{\\partial ^ 2 J(w)}{\\partial w \\partial w^T} = \\displaystyle\\sum_{i = 1}^{n} x_i^Tx_ip_i(1 - p_i) = X^TWX$  \n",
    "  \n",
    "뉴턴법의 이용한 비용 함수의 업데이트 공식  \n",
    "$\\rightarrow w^{(t + 1)} = w^{(t)} - \\Bigg [ \\bigg ( \\frac{\\partial ^ 2 J(w)}{\\partial w \\partial w^T} \\bigg ) \\frac{\\partial J(w)}{\\partial w} \\Bigg ]_{w = w^{(t)}} = w^{(t)} - (X^TWX)^{-1}X^T(p - y)$  \n",
    "  \n",
    " 뉴턴법을 이용한 이터레이션은 그 폭이 너무 커 발산할 수 있으므로 실질적인 추정을 위해 학습률을 감쇄하는 하이퍼파라미터 $v(0 < v < 1)$를 도입하는 수정 뉴턴법을 많이 사용함\n",
    " #### 수정 뉴턴법을 이용한 로지스틱 회귀 모델의 해 도출\n",
    " 초깃값\n",
    " 1. 파아미터 $w^{(0)}$을 랜덤하게 초기화하고 $v(0<v<1)$를 적절한 값으로 정의\n",
    " 2. $w^{(0)}$을 이용하여 각각의 $i$에 대하여 $p_i^{(0)} = \\frac{e^{x_iw^{(0)}}}{1 + e^{x_iw^{(0)}}}$를 계산한 후 $W^{(0)} = diag[p_i^{(0)}(1 - p_i^{(0)})]_{i = 1}^n$를 계산  \n",
    "  \n",
    "알고리즘\n",
    "- 정지 조건에 도달할 때까지 파라미터를 업데이트함  \n",
    "$w^{(t + 1)} = w^{(t)} - v(X^TW^{(t)}X)^{-1}X^T(p^{(t)} - y)$\n",
    "- $w^{(t)}$를 이용하여 $p^{(t)}$를 계산 후 다시 $p^{(t)}$를 이용하여 $W^{(t)}$를 계산한 다음, 계산한 값을 우변에 대입하여 $w^{(t + 1)}$을 계산\n",
    "\n",
    "#### 시간 복잡도\n",
    "- 로지스틱 회귀 모델의 학습은 최적화 방법을 나타내는 solver에 따라 크게 달라짐\n",
    "- 뉴턴법 : 이터레이션의 수 $i$에 따라 소요 시간이 비례하며 $i$가 클수록 수렴값의 정밀성(precision)이 높아짐\n",
    "- 뉴턴법을 이용한 수렴 시간 복잡도 : 헤시안 계산 때문에 다른 solver에 비해 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fffc7",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 모델의 심화 이론\n",
    "#### 규제\n",
    "- 최적화 문제 측면에서 로지스틱 회귀는 L1 or L2 규제(regularization)가 포함된 비용 함수를 구성할 수 있음\n",
    "- 규제 강도의 역수 하이퍼파라미터 C를 도입 (C에 큰 값을 설정할수록 규제의 효과가 약해지게 함)\n",
    "- L1 규제의 비용 함수 $J_{L1}(w) = \\parallel w \\parallel _1 + C \\bigg ( \\displaystyle\\sum_{i = 1}^{n} log(1 + e^{x_iw}) - y_ix_iw \\bigg )$\n",
    "- L2 규제의 비용 함수 $J_{L2}(w) = \\frac{1}{2}w^Tw + C \\bigg ( \\displaystyle\\sum_{i = 1}^{n} log(1 + e^{x_iw}) - y_ix_iw \\bigg )$\n",
    "- 엘라스틱 넷(elastic net) 규제 :  L1과 L2 규제를 조합한 것\n",
    "- 엘라스틱 넷 규제의 비용 함수 $J_{Elastic Net}(w) = \\rho \\parallel w \\parallel_1 + \\frac{1 - \\rho}{2}w^Tw + C \\bigg ( \\displaystyle\\sum_{i = 1}^{n} log(1 + e^{x_iw}) - y_ix_iw \\bigg )$\n",
    "    - $\\rho$ : L1 규제와 L2 규제의 효과를 조절하는 상수\n",
    "    - $\\rho$가 1일 때 L1 규제, 0일 때 L2 규제와 같음\n",
    "- 로지스틱 회귀 모델의 비용 함수는 컨벡스 함수 + 규제항 $\\parallel w \\parallel_1$과 $\\frac{1}{2}w^Tw$또한 컨벡스 함수  \n",
    "$\\rightarrow$ 규제항을 포함해도 로지스틱 회귀 모델의 비용 함수는 전역 최솟값(global minimum)을 찾기 위해 경사 하강법 적용 가능  \n",
    "\n",
    "#### 다중 클래스 분류\n",
    "- 다중 클래스 분류(multiclass classification) : 3개 이상의 클래스를 분류하는 문제\n",
    "- K개의 클래스 레이블 $C = {C_1, C_2, \\dots, C_K}$를 분류하는 다중 클래스 분류라고 가정\n",
    "\n",
    "OVR(one-vs-rest) 방법  \n",
    "- 각각의 $i = 1, 2, \\dots, K$에 대하여 입력 샘플이 레이블 $C_i$에 포함될지 아닐지에 대한 확률만 계산하는 이빈 분류 모델 $M_i$를 학습\n",
    "- 그 후 ${M_i | 1 \\leq i \\leq K}$의 결과를 종합하여 최종 클래스를 판정\n",
    "- 결과의 종합 방법은 각각의 판정 확률 중 가장 높은 방법을 택하는 방식 등을 사용할 수 있음  \n",
    "  \n",
    "OVO(one-vs-one) 방법  \n",
    "- OVR과 비슷하지만 $\\begin{pmatrix} n\\\\ 2 \\end{pmatrix} = \\frac{n(n - 1)}{2}$개의 클래스 쌍에 대한 이진 분류 모형을 각각 학습한 후 이에 대한 결과를 종합  \n",
    "  \n",
    "모델에 따라 고유한 확장법\n",
    "- 일부 머신러닝 모델에서는 모델의 원리를 이용해 자연스럽게 이진 클래스 분류를 다중 클래스 분류로 확장할 수 있음\n",
    "- 로지스틱 회귀 모델이나 MLP 기법 등은 이진 클래스 분류일 때 시그모이드 함수로 클래스 분류 확률을 생성\n",
    "- 다중 클래스 분류에서는 이를 확장한 소프트맥스 함수를 사용하여 여러 가지 클래스 분류 확률을 구할 수 있음  \n",
    "소프트맥스 함수\n",
    "- K개 클래스 분류 모델에서 사용됨\n",
    "- K개 실숫값을 K개의 확률값으로 단조 변환하고자 사용\n",
    "- $softmax(z)_i = \\frac{exp(z_i)}{\\sum_{i = 1}^{K} exp(z_i)}$\n",
    "- K개의 클래스 분류를 위해 시그모이드 함수 정의를 확장한 것  \n",
    "  \n",
    "  \n",
    "- OVR과 OVO는 이진 분류에서의 모델링을 여러 번 수행하고 종합하여 이를 다중 분류 모델로 확장하는 방법  \n",
    "$\\rightarrow$ 정보의 종합 방법만 잘 정의한다면 어느 머신러닝 기법에도 적용할 수 있음  \n",
    "- OVR이나 OVO와 달리 특정 모델에서만 적용할 수 있는 고유 확장법이 있을 수도 있음\n",
    "    - 클래스 수가 3개 이상일 때 시그모이드 함수 대신 소프트맥스 함수를 적용해 이항 로지스틱 회귀(binary logistic regression)를 확장한 다중 클래스 로지스틱 회귀 모델을 적용할 수 있음 $\\rightarrow$ 다항 로지스틱 회귀(multinomial logistic regression) 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700a37e",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 모델 구현하기\n",
    "- 비용 함수에 뉴턴법을 적용하는 간단한 형태를 이용하여 이항 로지스틱 회귀 모델을 구현\n",
    "- sklearn에서 제공하는 분류 데이터셋 중 데이터 규모가 비교적 큰 유방암 데이터셋 사용\n",
    "- 간단한 상황을 가정하고자 맨 앞의 세 피처만 사용하여 모델링 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96e4b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y = True, as_frame = False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1234)\n",
    "\n",
    "X_train, X_test = X_train[:, :3], X_test[:, :3]\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e50e1",
   "metadata": {},
   "source": [
    "- 최적화 알고리즘이 단순화된 만큼 수렴의 안정성을 위해 스케일링을 수행하는 것이 필요\n",
    "- 학습 데이터셋의 각 피처가 평균 0과 분산 1이 되도록 표준 스케일링(standard scaling)을 수행\n",
    "- 같은 평균과 표준편차를 똑같이 테스트 데이터셋에도 적용\n",
    "- 학습 데이터셋과 테스트 데이터셋에 각각 절편을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a3167d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = X_train.mean(axis = 0), X_train.std(axis = 0)\n",
    "\n",
    "X_train, X_test = (X_train - train_mean) / train_std, (X_test - train_mean) / train_std\n",
    "\n",
    "n, n_test = X_train.shape[0], X_test.shape[0]\n",
    "X_train = np.append(np.ones((n, 1)), X_train, axis = 1)\n",
    "X_test = np.append(np.ones((n_test, 1)), X_test, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87089e8",
   "metadata": {},
   "source": [
    "- 최대 이터레이션 횟수와 조기 종료 조건을 설정\n",
    "- 최대 10,000번을 반복, 파라미터 업데이트 크기가 유클리드 거리 기준으로 0.0001 이하가 되면 이터레이션을 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bfd1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "Tolerance = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec96a14",
   "metadata": {},
   "source": [
    "- 파라미터를 초기화\n",
    "- 편의상 모든 계수가 1인 상태를 초깃값으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f308d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_old = np.ones((4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4aada0",
   "metadata": {},
   "source": [
    "- 이터레이션 수행\n",
    "- W 행렬과 p 벡터를 정의한 후 이를 식에 대입하는 비교적 간단한 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57c1f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이터레이션 : 1000, 업데이트 크기 : 0.0014765820989698008\n",
      "이터레이션 : 2000, 업데이트 크기 : 0.0008288194947011249\n",
      "이터레이션 : 3000, 업데이트 크기 : 0.0005888637557052147\n",
      "이터레이션 : 4000, 업데이트 크기 : 0.0005016460961682245\n",
      "이터레이션 : 5000, 업데이트 크기 : 0.000480588720186066\n",
      "이터레이션 : 6000, 업데이트 크기 : 0.000492169075454186\n",
      "이터레이션 : 7000, 업데이트 크기 : 0.0005216582869967076\n",
      "이터레이션 : 8000, 업데이트 크기 : 0.0005614510237056337\n",
      "이터레이션 : 9000, 업데이트 크기 : 0.0006068663712749307\n",
      "이터레이션 : 9999, 업데이트 크기 : 0.0006545214496913837\n",
      "\n",
      "학습한 파라미터\n",
      "[[ 4.96797147e-01]\n",
      " [ 5.07895925e+00]\n",
      " [ 2.20184378e-03]\n",
      " [-5.65908436e+00]]\n"
     ]
    }
   ],
   "source": [
    "for cnt in range(1, max_iter):\n",
    "    W = np.zeros((n, n))\n",
    "    p = np.zeros((n, 1))\n",
    "    \n",
    "    for i in range(n):\n",
    "        xb = np.exp((X_train[i].reshape(1, -1) @ beta_old)[0][0])\n",
    "        pi = xb / (1 + xb)\n",
    "        W[i][i] = pi * (1 - pi)\n",
    "        p[i] = pi\n",
    "        \n",
    "    left = np.linalg.inv(X_train.T @ W @ X_train)\n",
    "    right = X_train.T @ (y_train - p)\n",
    "    \n",
    "    update = 0.0001 * (left @ right)\n",
    "    beta_new = beta_old + update\n",
    "    \n",
    "    if (np.linalg.norm(update) < Tolerance):\n",
    "        break\n",
    "        \n",
    "    if (cnt % 1000 == 0):\n",
    "        print(f'이터레이션 : {cnt}, 업데이트 크기 : {np.linalg.norm(update)}')\n",
    "        \n",
    "    beta_old = beta_new\n",
    "    \n",
    "print(f'이터레이션 : {cnt}, 업데이트 크기 : {np.linalg.norm(update)}')\n",
    "print()\n",
    "print('학습한 파라미터')\n",
    "print(beta_new)\n",
    "\n",
    "# 최대 이터레이션 10,000에 도달하면 이터레이션은 종료하고 업데이트 크기가 5,000번째 이터레이션을 기점으로 다시 증가\n",
    "# 파라미터 업데이트 시 학습률 크기를 단순화했지만\n",
    "# 이 때문에 이터레이션 5,000회 이후에는 상대적인 학습률의 크기가 필요 이상으로 커서 해가 발산하는 현상으로 추정\n",
    "# 실제로는 업데이트 크기가 계속 감소하다가 Tolerance 아래로 내려가 최대 이터레이션 도달 전에 종료하는 것이 이상적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305dcb3",
   "metadata": {},
   "source": [
    "- 학습한 파라미터로 학습 데이터셋과 테스트 데이터셋에서의 정확도(accrancy)를 계산\n",
    "- 학습 데이터셋 내의 각 샘플에 대해 루프를 돌며 옳은 예측치의 수를 세고, 이 수와 샘플 전체 수의 비율을 정확도로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bc77f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 정확도 :  84.51%\n",
      "테스트 데이터셋 정확도 :  76.06%\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "for i in range(X_train.shape[0]):\n",
    "    xb = np.exp((X_train[i].reshape(1, -1) @ beta_old)[0][0])\n",
    "    pi = xb / (1 + xb)\n",
    "    if (pi >= 0.5 and y_train[i] == 1) or (pi < 0.5 and y_train[i] == 0):\n",
    "        right += 1\n",
    "        \n",
    "print(f'학습 데이터셋 정확도 : {right / X_train.shape[0] * 100 : .2f}%')\n",
    "\n",
    "right = 0\n",
    "for i in range(X_test.shape[0]):\n",
    "    xb = np.exp((X_test[i].reshape(1, -1) @ beta_old)[0][0])\n",
    "    pi = xb / (1 + xb)\n",
    "    if (pi >= 0.5 and y_test[i] == 1) or (pi < 0.5 and y_test[i] == 0):\n",
    "        right += 1\n",
    "        \n",
    "print(f'테스트 데이터셋 정확도 : {right / X_test.shape[0] * 100 : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983e2f7",
   "metadata": {},
   "source": [
    "## 3. 패키지로 표현하기\n",
    "sklearn.linear_model.LogisticRegression 클래스\n",
    "- 로지스틱 회귀 모델을 구현\n",
    "- 이진 분류와 다중 클래스 분류를 모두 수행할 수 있음\n",
    "- L1, L2, 엘라스틱 넷 규제 또한 적용할 수 있음\n",
    "- solver 하이퍼파라미터에는 비용 함수 최적화 알고리즘으로 'liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga' 중 하나를 선택할 수 있음  \n",
    "  \n",
    "'newton-cg' : 컬레기울기법(conjugate gradient method)  \n",
    "- 희소 데이터셋(sparse dataset)에서 추천  \n",
    "  \n",
    "'lbfgs' : L-BFGS-B 알고리즘  \n",
    "- 작은 크기의 데이터셋에 추천\n",
    "- 데이터의 크기가 크다면 성능이 떨어질 수 있음 \n",
    "  \n",
    "'liblinear' : 최적화된 좌표 하강법(coordinate descent) 알고리즘  \n",
    "- 데이터의 크기가 작을 때 추천  \n",
    "  \n",
    "'sag' : SAG(stochastic average gradient descent) 알고리즘  \n",
    "- 샘플 개수와 피처 개수가 모두 큰 대형 데이터셋에서 빠르게 수렴\n",
    "- 빠른 수렴은 각 피처의 스케일이 비슷할 때만 보장되므로 sklearn.preprocessing 모듈의 클래스를 사용해 데이터를 전처리하는 것이 좋음\n",
    "  \n",
    "'saga' : SAG 알고리즘의 변형  \n",
    "- 'sag'와 마찬가지로 대형 데이터셋에서 잘 동작\n",
    "- 피처 스케일에 대한 정규화를 선행하는 것이 좋음  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339159d",
   "metadata": {},
   "source": [
    "다항 분류 + L2 규제 : 'lbfgs', 'newton-cg', 'sag', 'saga'  \n",
    "OVR + L2 규제 : 'liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'  \n",
    "다항 분류 + L1 규제 : 'saga'  \n",
    "OVR + L1 규제 : 'liblinear', 'saga'  \n",
    "엘라스틱 넷 규제 : 'saga'  \n",
    "규제 미부여 기능 : 'lbfgs', 'newton-cg', 'sag', 'saga'  \n",
    "\n",
    "- LogisticRegression 클래스에서의 각 solver의 옵션을 나타냄\n",
    "- 다항 분류 옵션 : 다중 클래스 분류 시 크로스 엔트로피 비용 함수(cross-entropy cost function)를 이용한 다항 로지스틱 회귀를 수행할 수 있는지를 나타냄\n",
    "- 다항 분류 옵션이 제공되지 않는다면 다중 클래스가 나올 경우 각각의 클래스마다 이진 분류를 OVR 방식으로 수행한 후 이 것을 종합하는 방식으로 모델이 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06527b36",
   "metadata": {},
   "source": [
    "LogisticRegression 클래스의 주요 하이퍼파라미터  \n",
    "penalty\n",
    "- 주요값 : 'l1', 'l2', 'elasticnet', 'none'\n",
    "- 기본값 : 'l2'\n",
    "- 의미 : 규제 페널티 선택\n",
    "    - 'l1' : L1 규제 적용\n",
    "    - 'l2' : L2 규제 적용\n",
    "    - 'elasticnet' : 엘라스틱 넷 규제 적용\n",
    "    - 'none' : 규제 미부여  \n",
    "  \n",
    "tol  \n",
    "- 주요값 : float > 0\n",
    "- 기본값 : 1e-4\n",
    "- 의미 : 학습 종료에 대한 허용 오차 설정\n",
    "  \n",
    "C \n",
    "- 주요값 : float > 0\n",
    "- 기본값 : 1.0\n",
    "- 의미 : 규제 페널티 크기의 역수\n",
    "  \n",
    "fit_intercept  \n",
    "- 주요값 : bool\n",
    "- 기본값 : True\n",
    "- 의미 : 절편 항 포함 여부를 결정\n",
    "  \n",
    "class_weight\n",
    "- 주요값 : None, dict, 'balance'\n",
    "- 기본값 : None\n",
    "- 의미 : 클래스별 가중치 부여 방법\n",
    "    - None : 동일 가중치\n",
    "    - dict : 사용자 지정 가중치\n",
    "    - 'balance' : 클래스 빈도(frequency)에 반비례한 가중치\n",
    "  \n",
    "random_state  \n",
    "- 주요값 : int\n",
    "- 기본값 : None\n",
    "- 의미 : solver가 'sag', 'saga', 'liblinear'일 때 데이터를 섞게 되는데, 이에 대한 랜덤성을 제어하고자 사용\n",
    "  \n",
    "solver  \n",
    "- 주요값 : 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'\n",
    "- 기본값 : 'lbfgs'\n",
    "- 의미 : 최적화 알고리즘의 선택, 강건성때문에 'lbfgs'가 기본 solver로 주어짐\n",
    "  \n",
    "max_iter  \n",
    "- 주요값 : int > 0\n",
    "- 기본값 : 100\n",
    "- 의미 : 이터레이션 횟수의 상한선\n",
    "  \n",
    "multi_class  \n",
    "- 주요값 : 'auto', 'ovr', 'multinomial'\n",
    "- 기본값 : 'auto'\n",
    "- 의미 : 다중 클래스 분류 방법을 선택 (클래스가 2개라면 옵션에 따른 결과 차이는 없음)\n",
    "    - 'ovr' : OVR 분류 수행\n",
    "    - 'multinomial' : 크로스 엔트로피 비용 함수 사용\n",
    "    - 'auto' : solver가 'liblinear'일 때만 'ovr'을 선택하고 나머지는 'multinomial'을 선택\n",
    "  \n",
    "l1_ratio  \n",
    "- 주요값 : None, float\n",
    "- 기본값 : None\n",
    "- 의미 : 엘라스틱 넷 규제가 포함된 비용 함수에서 $\\rho$에 해당함 (penalty가 'elasticnet'일 때만 유효)\n",
    "  \n",
    "실제 사용 시 활용 팁\n",
    "- 정확도(accrancy), 정밀도(precision), 재현율(recall) 등의 성능 개선 측면에서는 하이퍼파라미터 튜닝에 사용할 수 있는 경우의 수가 다른 모델과 비교하여 상대적으로 적음\n",
    "    - 비용 함수 자체를 변화하는 하이퍼파라미터를 중심으로 튜닝하는 것이 좋음\n",
    "- 규제항에 포함되지 않은 로지스틱 모델은 solver의 활용과 관련된 일부 경우를 제외하고 피처 스케일링에 거의 영향을 받지 않음\n",
    "    - 하지만 LogisticRegression 객체는 생성 시 규제항을 포함하며, 규제항은 계수 전체의 L1 합이나 L2 합의 크기를 규제하기 때문에 피처의 스케일이 다른 경우 모델 예측역에 큰 영향을 줄 수 있음\n",
    "    $\\rightarrow$ 피처의 스케일 차이가 클 경우에는 피처 스케일링을 선행해야 함\n",
    "    \n",
    "#### 피처 스케일링이 필요한 모델\n",
    "- 거리를 기반으로 동작하거나 비용 함수가 피처의 범위에 크게 영향을 크게 받는다면 모델보다 피처 스케일링을 먼저 진행하는 것이 필수\n",
    "- OLS, 트리, 랜덤 포레스트 모델 : 일반적으로 스케일링 여부에 예측력이 영향을 받지 않음\n",
    "- 로지스틱 회귀, 라쏘, 릿지 등 비용 함수에 규제항이 포함된 모델 : 스케일링에 따라 예측력이 달라지게 됨\n",
    "    - 규제항을 포함하면 개수 크기의 L1 혹은 L2 합 전체를 규제하게 됨 (비용 함수가 큰 계수의 값에 영향을 받기 때문)\n",
    "- 다층 퍼셉트론(multilayer perceptron, MLP) 등 비용 함수가 복잡하고 전역 최적값 탐색이 어려운 모델 : 스케일링 필수\n",
    "    - 비용 함수에 규제항이 없어도 비용 함수 자체를 잘 학습시키는 것이 어렵기때문\n",
    "- K-최근접 이웃(K-nearest neighbors, KNN), 군집 분석 모델 등 거리를 기반으로 동작 : 스케일링 필수\n",
    "    - 모델이 주어진 샘플과 가까운 샘플을 찾아내는 것처럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b754cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression 클래스 사용하기\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y = True, as_frame = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1234)\n",
    "\n",
    "X_train, X_test = X_train.iloc[:, :3], X_test.iloc[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed986d2",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀 모델을 학습하고 학습 데이터셋과 테스트 데이터셋에 각각 적용해 정확도를 산출\n",
    "- 이론적 단순화를 위해 규제 페널티의 역수 C를 100,000으로 설정 $\\rightarrow$ 규제 효과가 거의 발생하지 않도록\n",
    "- 규제가 없다면 피처 스케일링 여부는 모델의 성능에 영향을 끼치지 않음 $\\rightarrow$ 피처 스케일링 없이 모델의 학습과 평가를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86a3b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 정확도 :  93.18%\n",
      "테스트 데이터셋 정확도 :  87.23%\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state = 1234, C = 100000)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f'학습 데이터셋 정확도 : {(y_train == y_train_pred).sum() / len(y_train) * 100 : .2f}%')\n",
    "print(f'테스트 데이터셋 정확도 : {(y_test == y_pred).sum() / len(y_test) * 100 : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82900ffc",
   "metadata": {},
   "source": [
    "## 4. 로지스틱 회귀 모델 보충 수업\n",
    "### 피처 스케일링과 파이프라인\n",
    "- 피처 스케일링(feature scaling) : 각 피처의 범위를 비슷하게 변환하는 것\n",
    "    - 표준 스케일링(standard scaling) : 평균과 분산을 통일 (sklearn.preprocessing 모듈의 StandardScaler 클래스)\n",
    "    - 최소-최대 스케일링(min-max scaling) : 최솟값과 최댓값을 통일 (sklearn.preprocessing 모듈의 MinMaxScaler 클래스)\n",
    "- 피처 스케일링을 포함한 모든 전처리에서는 테스트 데이터에도 학습 데이터와 같은 기준(학습 데이터로 계산한 기준값)을 그대로 적용해야 함\n",
    "  \n",
    "- 파이프라인(pipeline) : 일련의 정의된 과정을 하나의 함수로 정의하여 사용할 수 있게 하는 것  \n",
    "  \n",
    "파이프라인 단계\n",
    "1. 데이터셋을 학습과 테스트 데이터셋으로 분할 : sklearn.model_selection의 train_test_split() 함수 활용\n",
    "2. 학습 데이터셋에 스케일링 적용 : 스케일링을 수행하는 클래스의 객체 scaler를 정의 후 scaler.fit() 함수 활용\n",
    "3. 모델 학습 수행 : 학습 모델 클래스의 객체 model을 생성 후 model.fit() 함수 활용\n",
    "4. 테스트 데이터셋에 학습 데이터셋 기준의 스케일링 적용 : scaler.transform() 함수 활용\n",
    "5. 학습된 모델에 스케일링한 피처를 입력하고 예측값 산출 : model.predict() 함수 활용\n",
    "6. 예측값과 목푯값을 비교하여 모델 성능 측정 : sklearn.metrics.accuracy_score() 등의 함수 활용\n",
    "  \n",
    "sklearn.pipeline.Pipeline 클래스를 사용한 파이프라인 단순화\n",
    "1. 데이터셋을 학습 데이터셋과 테스트 데이터셋으로 분할\n",
    "2. Pipeline 클래스를 이용해 스케일링과 학습을 순서대로 수행하도록 정의한 객체 pipe 생성\n",
    "3. 학습 데이터셋으로 pipe 객체 학습\n",
    "4. pipe 객체에 테스트 데이터셋을 적용하여 예측값 산출\n",
    "5. 예측값과 목푯값을 비교하여 모델 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5223987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 스케일링과 학습 파이프라인\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y = True, as_frame = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26bbd9",
   "metadata": {},
   "source": [
    "- 표준 스케일링과 로지스틱 회귀 모델링을 차례대로 수행하는 Pipeline 클래스의 객체 pipe를 정의\n",
    "- 이를 사용해 학습을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa380d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82108\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('clf', LogisticRegression(random_state=1234, solver='sag'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "clf = LogisticRegression(random_state = 1234, solver = 'sag')\n",
    "pipe = Pipeline(steps = [(\"scaler\", scaler), (\"clf\", clf)])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465818e1",
   "metadata": {},
   "source": [
    "- 학습된 pipe 객체에 예측 데이터셋을 입력하여 예측값을 구하고 정확도를 산출\n",
    "- pipe 객체의 predict() 메서드를 사용할 때는 학습 데이터를 기준으로 얻은 스케일링 값과 모델 파라미터가 적용된다는 점에 유의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfb3abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 :  94.68%\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(f'정확도 : {(y_pred == y_test).mean() * 100 : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29325fee",
   "metadata": {},
   "source": [
    "### 통계 모델로서의 로지스틱 회귀 기법\n",
    "- 머신러닝 관점이 아닌 통계학 관점에서 로지스틱 회귀 모델의 최대 가능도 추정(maximum likelihood estimation, MLE)을 통한 해를 구할 수도 있음\n",
    "  \n",
    "#### MLE\n",
    "- 주어진 데이터를 이용해 가능도(likelihood)를 최대화하는 파라미터를 찾는 통계 기법\n",
    "- 최대 가능도 추정량(maximum likelihood esimator) : 결합 가능도(joint likelihood)를 최대화하는 w의 추정량(estimator)\n",
    "    - 관측값 $x_i$ $(1 \\leq i \\leq n)$ : 확률밀도함수(probability density function) $f_X(x | w_1, w_2, \\dots, w_p)$에서 추출한 독립 항등 분포(independent and identically distributed, IID) 샘플\n",
    "    - $w = [w_1, w_2, \\dots, w_p]$ : 추정해야 할 파라미터   \n",
    "$\\hat{w} = argmax_w \\displaystyle\\prod_{i=1}^n logf_X(x_i | w)$\n",
    "\n",
    "- 로그 함수가 단조 증가 함수 $\\rightarrow$ 로그-결합 가능도(log-joint likelihood)를 최대화하는 문제와 같음  \n",
    "$\\hat{w} = argmax_w \\displaystyle\\sum_{i=1}^n logf_X(x_i | w)$  \n",
    "  \n",
    "정규 분포(normal distribution)에서의 평균 $\\mu$와 분산 $\\sigma^2$에 대한 MLE\n",
    "- IID 정규 분포 $N(\\mu, \\sigma^2)$를 따르는 $X_i \\; (1 \\leq i \\leq n)$의 확률밀도함수 $f_{X_i}(x;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}$  \n",
    "$\\rightarrow$ 로그-결합 가능도 $l(x_1, x_2, \\dots, x_n;\\mu, \\sigma^2) = \\displaystyle\\sum_{i = 1}^{n} logf_{X_i}(x_i;\\mu, \\sigma^2) = -\\frac{1}{2\\sigma^2} \\displaystyle\\sum_{i = 1}^{n} (x_i - \\mu)^2 - \\frac{1}{2} nlog(2\\pi\\sigma^2)$  \n",
    "- 로그-결합 가능도를 $\\mu$대해 미분  \n",
    "$\\frac{\\partial l}{\\partial\\mu} = -\\frac{1}{2\\sigma^2} \\displaystyle\\sum_{i = 1}^{n} 2(\\mu - x_i) = 0$ $\\quad \\rightarrow \\quad$ $\\hat{\\mu} = \\frac{1}{n} \\displaystyle\\sum_{i = 1}^{n} x_i$\n",
    "- 로그-결합 가능도를 $\\sigma^2$에 대해 미분하고 $\\hat{\\mu}$을 대입  \n",
    "$\\frac{\\partial l}{\\partial\\sigma^2} = \\frac{1}{2\\sigma^4} \\displaystyle\\sum_{i = 1}^{n} (x_i - \\mu)^2 - \\frac{n}{2\\sigma^2} = 0$ $\\quad \\rightarrow \\quad$ $\\hat{\\sigma}^2 = \\frac{1}{n} \\displaystyle\\sum_{i = 1}^{n} (x_i - \\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435f839",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀 모델의 MLE\n",
    "- 베르누이 분포 Bernoulli(p)에서의 확률 분포  \n",
    "    - $Pr(Y = 0) \\quad = \\quad 1 - p$\n",
    "    - $Pr(Y = 1) \\quad = \\quad p$\n",
    "- 베르누이 분포의 확률 질량 함수(probability mass function, PMF)  \n",
    "$Pr(Y = y_i) = p(y_i)^{y_i}(1 - p(y_i))^{1 - y_i},\\; y_i = 0 \\,or\\, 1$\n",
    "- 로지스틱 회귀 모델에서의 $i$번째 관측값의 로그 가능도 계산  \n",
    "$l_i(w|x_i) = y_ilogp(y_i) + (1 - y_i)log(1 - p(y_i))$\n",
    "- n개의 관측값에 대한 결합 로그 가능도 = 각각의 로그 가능도 합  \n",
    "$l(w|x_1, \\dots, x_n) = \\displaystyle\\sum_{i = 1}^{n} l_i(w|x_i) = \\displaystyle\\sum_{i = 1}^{n} \\big ( (1 - y_i)log(1 - p(y_i) + y_ilogp(y_i) \\big ) = \\displaystyle\\sum_{i = 1}^{n} \\Bigg ( y_ilog\\frac{p(y_i)}{1 - p(y_i)} + log(1 - p(y_i)) \\Bigg )$\n",
    "- 로지스틱 회귀 모델에서의 결합 가능도  \n",
    "$l(w|x_1, \\dots, x_n) = \\displaystyle\\sum_{i = 1}^{n} \\Big ( y_ix_iw - log(1 + e^{x_iw})\\Big )$\n",
    "  \n",
    "- 결합 로그 가능도가 최댓값이 되는 $w$rk 로지스틱 회귀 모델의 MLE\n",
    "- 결합 로그 가능도의 값 : 비용 함수 $J(w)$와 부호는 반대, 정확히 같은 절댓값  \n",
    "$\\rightarrow$ $l(w|x_1, \\dots, x_n)$의 최댓값을 찾는 것 = $J(w)$의 최솟값을 찾는 것\n",
    "- 컨케이브 함수(concave function)의 최댓값을 찾는 것 = 컨백스 함수(convex function)의 최솟값을 찾는 방법\n",
    "  \n",
    "규제가 없는 로직스틱 회귀 모델에 통계학 가정을 추가할 때 얻은 파라미터의 신뢰 구간과 유의 확률을 계산할 수 있음\n",
    "- statsmodels 패키지 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799aad25",
   "metadata": {},
   "source": [
    "### 교차검증법\n",
    "sklearn.linear_model.LogisticRegressionCV 클래스\n",
    "- 교차검증법을 동반한 로지스틱 회귀를 수행하고 규제 강도의 역수 C 등 최적의 하이퍼파라미터를 탐색\n",
    "- 'newton-cg', 'sag', 'saga', 'lbfgs' solver는 warm_start를 사용할 경우 고차원의 밀집 데이터셋에서 빠르게 수행될 수 있음\n",
    "\n",
    "### 피처 선택법\n",
    "- 주어진 피처의 집합에서 일부틔 피처만 모델에 포함하는 피처 선택법(feature selection)\n",
    "- 로지스틱 회귀 모델의 비용 함수에 L1 규제를 추가할 때 예측에 기여하는 정도가 적은 일부 피처의 계수가 0이 되며, 해당 피처는 예측 모델에 사용하지 않게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194f33d",
   "metadata": {},
   "source": [
    "### 3장 되새김 문제\n",
    "#### 1. LogisticRegression 클래스 적용하기\n",
    "- 붓꽃 데이터셋을 다중 클래스 분류 모델로 학습하고 그 결과를 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dba2a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지와 데이터셋 정의\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "X, y = load_iris(return_X_y = True, as_frame = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15df435",
   "metadata": {},
   "source": [
    "- 최소-최대 스케일러(min-max scaler)를 학습 데이터셋 피처에 적용하여 피처 스케일링을 수행하고 그 결과를 LogisticRegression 클래스로 학습하라.\n",
    "- SAGA 알고리즘으로 해를 최적화하도록 하며, 다중 클래스 분류를 위해 OVR 방법을 사용하도록한다.\n",
    "- 결과 재현성을 위해 random_state는 1234로 설정한다.\n",
    "- 그 후 학습한 결과를 테스트 데이터셋에 적용하고 target이 1일 때의 정밀도(precision)를 구하라.  \n",
    "$Precision^{(target = 1)} = \\frac{목푯값이 1이고 예측값도 1인 샘플의 수}{예측값이 1인 샘플의 수}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04089791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정밀도 :  94.12%\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "clf = LogisticRegression(random_state = 1234, solver = 'saga')\n",
    "pipe = Pipeline(steps = [(\"scaler\", scaler), (\"clf\", clf)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "pre = precision_score(y_test, y_pred, average = None)\n",
    "print(f'정밀도 : {pre[1] * 100 : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15ddfe",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀 모델용 데이터셋 생성하기\n",
    "- 로지스틱 회귀 모델을 따르는 적당한 데이터셋을 생성하고 이를 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6a3b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 정의\n",
    "# 데이터셋이 정의되어 있다고 가정\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n = 300\n",
    "np.random.seed(1234)\n",
    "X1 = np.random.normal(0, 1, size = n)\n",
    "X2 = np.random.normal(0, 1, size = n)\n",
    "X3 = np.random.normal(0, 1, size = n)\n",
    "X4 = np.random.normal(0, 1, size = n)\n",
    "X5 = np.random.normal(0, 1, size = n)\n",
    "\n",
    "X = pd.DataFrame({'X1' : X1, 'X2' : X2, 'X3' : X3, 'X4' : X4, 'X5' : X5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d766e",
   "metadata": {},
   "source": [
    "테스트할 로지스틱 회귀 모델의 조건\n",
    "1. 절편을 모델에 포함한다.\n",
    "2. 재현성을 위해 random_state = 1234로 설정한다.\n",
    "  \n",
    "다음 조건을 만족하는 목푯값 y를 생성하라.\n",
    "1. 절편과 X1, X2의 계수는 모두 음수값이 얻어진다.\n",
    "2. X3, X4, X5의 계수는 모두 양수값이 얻어진다.\n",
    "3. 모델의 학습 데이터 내 정확도는 75%에서 80% 사이로 얻어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7c7f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_state = []\n",
    "intercept = -2\n",
    "beta = np.array([[-2, -2, 3, 4, 5]]).reshape(-1, 1)\n",
    "\n",
    "np.random.seed(1111)\n",
    "for i in range(n):\n",
    "    xb = np.exp((intercept + (X.iloc[i].values.reshape(1, -1) @ beta)[0][0]) + np.random.normal(0, 7.5))\n",
    "    pi = xb / (1 + xb)\n",
    "    if (pi >= 0.5):\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 0\n",
    "        \n",
    "    y_state.append(y)\n",
    "    \n",
    "y = pd.Series(y_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3067aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편 : -0.5786869408956349\n",
      "계수 : [-0.52203159 -0.51494024  0.71392631  1.05416056  1.06007824]\n",
      "정확도 :  78.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state = 1234)\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "print(f'절편 : {clf.intercept_[0]}')\n",
    "print(f'계수 : {clf.coef_[0]}')\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print(f'정확도 : {(y == y_pred).mean() * 100 : .2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
